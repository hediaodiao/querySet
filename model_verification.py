import json
import os
import numpy as np
import torch
import open_clip
from PIL import Image
from collections import defaultdict
import time
from datetime import datetime
import pickle
from pathlib import Path
import torchvision.transforms as transforms
import cv2

# é…ç½®é€‰é¡¹ï¼šæ˜¯å¦é‡æ–°ç”Ÿæˆå›¾ç‰‡ç‰¹å¾ç¼“å­˜
# è®¾ç½®ä¸º True æ—¶ä¼šæ¸…é™¤ç°æœ‰ç¼“å­˜å¹¶é‡æ–°ç”Ÿæˆç‰¹å¾ï¼›è®¾ç½®ä¸º False æ—¶ä½¿ç”¨ç°æœ‰ç¼“å­˜
REBUILD_FEATURE_CACHE = False

# é…ç½®é€‰é¡¹ï¼šæ˜¯å¦åªä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œä¸è¿›è¡Œåœ¨çº¿ä¸‹è½½
# è®¾ç½®ä¸º True æ—¶åªä½¿ç”¨æœ¬åœ°æ¨¡å‹ï¼Œå¦‚æœæœ¬åœ°æ¨¡å‹ä¸å­˜åœ¨åˆ™ç»ˆæ­¢ç¨‹åº
# è®¾ç½®ä¸º False æ—¶å…è®¸ä»ç½‘ç»œä¸‹è½½æ¨¡å‹ï¼ˆé¦–æ¬¡è¿è¡Œéœ€è¦ç½‘ç»œè¿æ¥ï¼‰
USE_LOCAL_MODEL_ONLY = False


class OpenCLIPFeatureExtractor:
    """
    ä½¿ç”¨OpenCLIPæ¨¡å‹æå–å›¾ç‰‡ç‰¹å¾
    æ”¯æŒç¼“å­˜æœºåˆ¶å’Œæ‰¹é‡å¤„ç†
    """
    
    def __init__(self, cache_dir="./model_cache/features", device=None):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # å¦‚æœREBUILD_FEATURE_CACHEä¸ºTrueï¼Œåˆ™æ¸…é™¤ç°æœ‰ç¼“å­˜
        if REBUILD_FEATURE_CACHE:
            self.clear_cache()
        
        # è®¾ç½®è®¾å¤‡
        self.device = device or torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½OpenCLIPæ¨¡å‹
        model_name = "timm/vit_base_patch32_clip_224.laion2b_e16"
        
        print("ğŸ”„ åŠ è½½OpenCLIPæ¨¡å‹...")
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰æœ¬åœ°æ¨¡å‹æ–‡ä»¶
            local_model_path = "./model_cache/timm/vit_base_patch32_clip_224.laion2b_e16"
            
            # æ£€æŸ¥æœ¬åœ°æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆæ£€æŸ¥ç‰¹å®šçš„æ¨¡å‹æƒé‡æ–‡ä»¶ï¼‰
            config_path = os.path.join(local_model_path, "open_clip_config.json")
            safetensors_path = os.path.join(local_model_path, "open_clip_model.safetensors")
            bin_path = os.path.join(local_model_path, "pytorch_model.bin")
            model_path = os.path.join(local_model_path, "model.safetensors")
            
            local_model_files_exist = (
                os.path.exists(config_path) and
                (os.path.exists(safetensors_path) or 
                 os.path.exists(bin_path) or
                 os.path.exists(model_path))
            )
            
            if local_model_files_exist:
                print(f"ğŸ“ å‘ç°æœ¬åœ°æ¨¡å‹æ–‡ä»¶: {local_model_path}")
                
                # åŠ è½½æœ¬åœ°æ¨¡å‹ - ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼
                self.model, _, self.preprocess = open_clip.create_model_and_transforms(
                    model_name='ViT-B-32',
                    pretrained='laion2b_e16',
                    cache_dir='./model_cache'
                )
                print("âœ… ä»æœ¬åœ°åŠ è½½OpenCLIPæ¨¡å‹æˆåŠŸ")
            else:
                if USE_LOCAL_MODEL_ONLY:
                    print(f"âŒ æœªæ‰¾åˆ°æœ¬åœ°æ¨¡å‹æ–‡ä»¶: {local_model_path}")
                    print("âŒ ç”±äºUSE_LOCAL_MODEL_ONLY=Trueï¼Œé¡¹ç›®ç»ˆæ­¢")
                    raise FileNotFoundError(f"æœ¬åœ°æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {local_model_path}")
                else:
                    # å°è¯•ä¸‹è½½æ¨¡å‹
                    print(f"ğŸŒ æ­£åœ¨ä¸‹è½½OpenCLIPæ¨¡å‹: {model_name}")
                    self.model, _, self.preprocess = open_clip.create_model_and_transforms(
                        model_name='ViT-B-32',
                        pretrained='laion2b_e16',
                        cache_dir='./model_cache'
                    )
                    print("âœ… ä¸‹è½½å¹¶åŠ è½½OpenCLIPæ¨¡å‹æˆåŠŸ")
                
        except Exception as e:
            print(f"âŒ åŠ è½½OpenCLIPæ¨¡å‹å¤±è´¥: {e}")
            print("ğŸ’¡ è¯·ç¡®ä¿å·²å®‰è£…open_clip_torch: pip install open_clip_torch")
            raise
        
        # ç§»åŠ¨æ¨¡å‹åˆ°è®¾å¤‡
        self.model = self.model.to(self.device)
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        self.tokenizer = open_clip.get_tokenizer('ViT-B-32')
        
    def get_feature_cache_path(self, image_path):
        """è·å–ç‰¹å¾ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
        # ä½¿ç”¨å›¾åƒè·¯å¾„çš„å“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜æ–‡ä»¶å
        image_hash = hash(image_path) % (10**16)  # é™åˆ¶å“ˆå¸Œé•¿åº¦
        return self.cache_dir / f"{image_hash}.pkl"
    
    def load_cached_feature(self, image_path):
        """ä»ç¼“å­˜åŠ è½½ç‰¹å¾"""
        cache_path = self.get_feature_cache_path(image_path)
        if cache_path.exists():
            try:
                with open(cache_path, 'rb') as f:
                    return pickle.load(f)
            except:
                return None
        return None
    
    def save_feature_to_cache(self, image_path, feature):
        """ä¿å­˜ç‰¹å¾åˆ°ç¼“å­˜"""
        cache_path = self.get_feature_cache_path(image_path)
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(feature, f)
        except Exception as e:
            print(f"âš ï¸  ä¿å­˜ç‰¹å¾ç¼“å­˜å¤±è´¥: {e}")
    
    def clear_cache(self):
        """æ¸…é™¤ç‰¹å¾ç¼“å­˜"""
        import shutil
        if self.cache_dir.exists():
            print(f"ğŸ—‘ï¸  æ¸…é™¤ç‰¹å¾ç¼“å­˜ç›®å½•: {self.cache_dir}")
            shutil.rmtree(self.cache_dir)
            self.cache_dir.mkdir(parents=True, exist_ok=True)
            print("âœ… ç‰¹å¾ç¼“å­˜å·²æ¸…é™¤")
        else:
            print(f"â„¹ï¸  ç¼“å­˜ç›®å½•ä¸å­˜åœ¨: {self.cache_dir}")
    
    def preprocess_image(self, image_path):
        """é¢„å¤„ç†å•å¼ å›¾ç‰‡"""
        try:
            image = Image.open(image_path).convert('RGB')
            return self.preprocess(image)
        except Exception as e:
            print(f"âš ï¸  é¢„å¤„ç†å›¾ç‰‡å¤±è´¥ {image_path}: {e}")
            return None
    
    def extract_features_batch(self, image_paths, batch_size=8):
        """æ‰¹é‡æå–ç‰¹å¾"""
        features = {}
        
        # åˆ†æ‰¹å¤„ç†
        for i in range(0, len(image_paths), batch_size):
            batch_paths = image_paths[i:i+batch_size]
            
            # åŠ è½½å¹¶é¢„å¤„ç†æ‰¹æ¬¡å›¾ç‰‡
            batch_images = []
            valid_paths = []
            
            for img_path in batch_paths:
                full_path = os.path.join("./Pic", img_path)
                if os.path.exists(full_path):
                    # å¦‚æœè®¾ç½®äº†å¼ºåˆ¶é‡å»ºç¼“å­˜ï¼Œåˆ™è·³è¿‡åŠ è½½ç¼“å­˜
                    if hasattr(self, '_force_rebuild_cache') and self._force_rebuild_cache:
                        cached_feature = None
                    else:
                        cached_feature = self.load_cached_feature(full_path)
                    
                    if cached_feature is not None:
                        features[img_path] = cached_feature
                        continue
                    
                    preprocessed_img = self.preprocess_image(full_path)
                    if preprocessed_img is not None:
                        batch_images.append(preprocessed_img)
                        valid_paths.append(img_path)
            
            # æ‰¹é‡å¤„ç†æœªç¼“å­˜çš„å›¾ç‰‡
            if batch_images:
                # è½¬æ¢ä¸ºtensorå¹¶ç§»åŠ¨åˆ°è®¾å¤‡
                batch_tensor = torch.stack(batch_images).to(self.device)
                
                # æå–ç‰¹å¾
                with torch.no_grad():
                    if self.device.type == 'cuda':
                        with torch.cuda.amp.autocast():
                            image_features = self.model.encode_image(batch_tensor)
                    else:
                        image_features = self.model.encode_image(batch_tensor)
                    image_features /= image_features.norm(dim=-1, keepdim=True)  # å½’ä¸€åŒ–
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶ä¿å­˜åˆ°ç¼“å­˜
                image_features_np = image_features.cpu().numpy()
                
                for j, img_path in enumerate(valid_paths):
                    feature = image_features_np[j]
                    features[img_path] = feature
                    
                    # ä¿å­˜åˆ°ç¼“å­˜
                    full_path = os.path.join("./Pic", img_path)
                    self.save_feature_to_cache(full_path, feature)
        
        return features


class OptimizedModelVisualVerifier:
    """
    ä¼˜åŒ–çš„æ¨¡å‹è§†è§‰éªŒè¯å™¨
    ä½¿ç”¨OpenCLIPæ¨¡å‹ã€ç¼“å­˜æœºåˆ¶å’Œæ‰¹é‡å¤„ç†
    """
    
    def __init__(self, base_path="./Pic", cache_dir="./model_cache/features", batch_size=8):
        self.base_path = base_path
        self.batch_size = batch_size
        
        # åˆå§‹åŒ–ç‰¹å¾æå–å™¨
        self.feature_extractor = OpenCLIPFeatureExtractor(cache_dir=cache_dir)
        
        print("ğŸ’¡ åˆå§‹åŒ–ä¼˜åŒ–æ¨¡å‹è§†è§‰éªŒè¯å™¨")
        
    def calculate_similarity(self, feat1, feat2):
        """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
        if feat1 is None or feat2 is None:
            return 0.0
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        similarity = np.dot(feat1, feat2) / (np.linalg.norm(feat1) * np.linalg.norm(feat2))
        return max(0.0, similarity)  # ç¡®ä¿éè´Ÿ
    
    def verify_similarity_annotations(self, annotations_file="similarity_annotations.json", 
                                     output_path="similarity_annotations_verification_report.json"):
        """éªŒè¯ç›¸ä¼¼åº¦æ ‡æ³¨æ–‡ä»¶ - ä¼˜åŒ–ç‰ˆ"""
        print("ğŸ” å¼€å§‹éªŒè¯ç›¸ä¼¼åº¦æ ‡æ³¨æ–‡ä»¶...")
        
        # è¯»å–æ ‡æ³¨æ–‡ä»¶
        with open(annotations_file, 'r', encoding='utf-8') as f:
            annotations = json.load(f)
        
        print(f"ğŸ“Š æ€»å…± {len(annotations)} ä¸ªäº§å“ç»„å¾…éªŒè¯")
        
        # æŒ‰äº§å“IDåˆ†ç»„
        product_groups = defaultdict(list)
        for entry in annotations:
            product_id = entry.get('product_id', 'unknown')
            product_groups[product_id].append(entry)
        
        product_ids = list(product_groups.keys())
        print(f"ğŸ“¦ å‘ç° {len(product_ids)} ä¸ªä¸åŒçš„äº§å“ID")
        
        verified_annotations = []
        confirmed_groups = []
        split_suggestions = []
        merge_suggestions = []
        
        # é¦–å…ˆæå–æ‰€æœ‰å›¾ç‰‡çš„ç‰¹å¾ï¼ˆæ‰¹é‡å¤„ç†ï¼‰
        print("\nğŸ”„ æ‰¹é‡æå–æ‰€æœ‰å›¾ç‰‡ç‰¹å¾...")
        all_image_paths = set()
        for entries in product_groups.values():
            for entry in entries:
                all_image_paths.add(entry['query_image'])
                all_image_paths.update(entry['relevant_images'])
        
        all_image_paths = list(all_image_paths)
        print(f"æ€»å…±éœ€è¦å¤„ç† {len(all_image_paths)} å¼ å”¯ä¸€å›¾ç‰‡")
        
        # æ‰¹é‡æå–ç‰¹å¾
        all_features = self.feature_extractor.extract_features_batch(
            all_image_paths, 
            batch_size=self.batch_size
        )
        print(f"âœ… ç‰¹å¾æå–å®Œæˆï¼Œå…± {len(all_features)} ä¸ªç‰¹å¾")
        
        # 1. æ£€æŸ¥ç»„å†…ä¸€è‡´æ€§
        print("\nå¼€å§‹æ£€æŸ¥ç»„å†…ä¸€è‡´æ€§...")
        for idx, (product_id, entries) in enumerate(product_groups.items()):
            print(f"  å¤„ç†äº§å“ç»„ {idx+1}/{len(product_ids)}: {product_id}")
            
            # è®¡ç®—query_imageä¸relevant_imagesä¹‹é—´çš„ç›¸ä¼¼åº¦
            all_intra_similarities = []
            low_similarity_images = []  # å­˜å‚¨ä½ç›¸ä¼¼åº¦çš„å›¾ç‰‡
            
            for entry in entries:
                query_img = entry['query_image']
                
                # è®¡ç®—query_imageä¸æ¯ä¸ªrelevant_imageçš„ç›¸ä¼¼åº¦
                entry_similarities = []
                for rel_img in entry['relevant_images']:
                    if query_img in all_features and rel_img in all_features:
                        sim = self.calculate_similarity(all_features[query_img], all_features[rel_img])
                        entry_similarities.append(sim)
                        
                        # å¦‚æœç›¸ä¼¼åº¦ä½äºé˜ˆå€¼ï¼Œæ ‡è®°ä¸ºéœ€è¦äººå·¥å®¡æ ¸
                        if sim < 0.5:  # è®¾å®šé˜ˆå€¼ä¸º0.5
                            low_similarity_images.append({
                                'query_image': query_img,
                                'relevant_image': rel_img,
                                'similarity': sim
                            })
                
                # è®¡ç®—è¯¥entryçš„å¹³å‡ç›¸ä¼¼åº¦
                avg_entry_similarity = np.mean(entry_similarities) if entry_similarities else 1.0
                all_intra_similarities.append(avg_entry_similarity)
            
            # è®¡ç®—æ•´ä¸ªäº§å“çš„å¹³å‡ç›¸ä¼¼åº¦
            avg_intra_similarity = np.mean(all_intra_similarities) if all_intra_similarities else 1.0
            print(f"  ç»„å†…å¹³å‡ç›¸ä¼¼åº¦: {avg_intra_similarity:.3f}")
            
            # æ ¹æ®ç›¸ä¼¼åº¦å†³å®šå¤„ç†æ–¹å¼
            if avg_intra_similarity >= 0.85:  # é«˜ç›¸ä¼¼åº¦ï¼Œç¡®è®¤åˆ†ç»„
                for entry in entries:
                    entry['verification_status'] = 'confirmed_high_sim'
                    entry['intra_similarity'] = avg_intra_similarity
                    entry['low_similarity_images'] = []  # æ²¡æœ‰ä½ç›¸ä¼¼åº¦å›¾ç‰‡
                    verified_annotations.append(entry)
                confirmed_groups.append(product_id)
                print(f"  â†’ ç¡®è®¤åˆ†ç»„ {product_id} (ç›¸ä¼¼åº¦: {avg_intra_similarity:.3f})")
            elif avg_intra_similarity < 0.5:  # ä½ç›¸ä¼¼åº¦ï¼Œå»ºè®®æ‹†åˆ†
                split_suggestions.append({
                    'product_id': product_id,
                    'avg_similarity': avg_intra_similarity,
                    'image_count': sum([len(entry['relevant_images']) + 1 for entry in entries]),  # æ€»å›¾ç‰‡æ•°
                    'images': low_similarity_images,  # åŒ…å«ä½ç›¸ä¼¼åº¦å›¾ç‰‡ä¿¡æ¯
                    'low_similarity_details': low_similarity_images
                })
                # æš‚æ—¶ä¿ç•™åŸæ¡ç›®ï¼Œç­‰å¾…äººå·¥å¤„ç†
                for entry in entries:
                    entry['verification_status'] = 'needs_split'
                    entry['intra_similarity'] = avg_intra_similarity
                    entry['low_similarity_images'] = low_similarity_images  # æ·»åŠ ä½ç›¸ä¼¼åº¦å›¾ç‰‡ä¿¡æ¯
                    verified_annotations.append(entry)
                print(f"  â†’ å»ºè®®æ‹†åˆ† {product_id} (ç›¸ä¼¼åº¦: {avg_intra_similarity:.3f}), ä½ç›¸ä¼¼åº¦å›¾ç‰‡æ•°: {len(low_similarity_images)}")
            else:
                # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼Œä¿æŒåŸæ ·ï¼Œäººå·¥å®¡æ ¸
                for entry in entries:
                    entry['verification_status'] = 'needs_review'
                    entry['intra_similarity'] = avg_intra_similarity
                    entry['low_similarity_images'] = low_similarity_images  # æ·»åŠ ä½ç›¸ä¼¼åº¦å›¾ç‰‡ä¿¡æ¯
                    verified_annotations.append(entry)
                print(f"  â†’ éœ€è¦äººå·¥å®¡æ ¸ {product_id} (ç›¸ä¼¼åº¦: {avg_intra_similarity:.3f}), ä½ç›¸ä¼¼åº¦å›¾ç‰‡æ•°: {len(low_similarity_images)}")
        
        # 2. æ£€æŸ¥ç»„é—´åˆå¹¶å»ºè®®ï¼ˆä»…æ¯”è¾ƒqueryå›¾ç‰‡ï¼‰
        print("\nå¼€å§‹æ£€æŸ¥ç»„é—´åˆå¹¶å»ºè®®ï¼ˆä»…æ¯”è¾ƒqueryå›¾ç‰‡ï¼‰...")
        total_comparisons = len(product_ids) * (len(product_ids) - 1) // 2
        print(f"æ€»å…±éœ€è¦è¿›è¡Œ {total_comparisons} æ¬¡ç»„é—´ç›¸ä¼¼åº¦æ¯”è¾ƒ")
        
        product_ids = list(product_groups.keys())
        processed_count = 0
        start_time = time.time()
        
        for i in range(len(product_ids)):
            for j in range(i + 1, len(product_ids)):
                pid1, pid2 = product_ids[i], product_ids[j]
                
                # æ˜¾ç¤ºè¿›åº¦
                processed_count += 1
                if processed_count % max(1, total_comparisons // 20) == 0:  # æ¯5%æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                    elapsed = time.time() - start_time
                    eta = (elapsed / processed_count) * (total_comparisons - processed_count)
                    print(f"  è¿›åº¦: {processed_count}/{total_comparisons} "
                          f"({processed_count/total_comparisons*100:.1f}%) "
                          f"[è€—æ—¶: {elapsed:.1f}s, é¢„è®¡å‰©ä½™: {eta:.1f}s]")
                
                # è·å–ä¸¤ä¸ªäº§å“çš„queryå›¾ç‰‡ï¼ˆç¬¬ä¸€ä¸ªå›¾ç‰‡ï¼‰
                query_img1 = product_groups[pid1][0]['query_image']
                query_img2 = product_groups[pid2][0]['query_image']
                
                # æ¯”è¾ƒqueryå›¾ç‰‡çš„ç›¸ä¼¼åº¦
                if query_img1 in all_features and query_img2 in all_features:
                    similarity = self.calculate_similarity(all_features[query_img1], all_features[query_img2])
                    
                    # å¦‚æœç»„é—´ç›¸ä¼¼åº¦é«˜ï¼Œå»ºè®®åˆå¹¶
                    if similarity >= 0.75:  # é«˜ç›¸ä¼¼åº¦ï¼Œå»ºè®®åˆå¹¶
                        merge_suggestions.append({
                            'group_a': pid1,
                            'group_b': pid2,
                            'similarity_score': similarity,
                            'group_a_size': len([img for entry in product_groups[pid1] 
                                               for img in [entry['query_image']] + entry['relevant_images']]),
                            'group_b_size': len([img for entry in product_groups[pid2] 
                                               for img in [entry['query_image']] + entry['relevant_images']]),
                            'group_a_query': query_img1,
                            'group_b_query': query_img2
                        })
        
        print(f"\nâœ… å®Œæˆæ‰€æœ‰ç›¸ä¼¼åº¦éªŒè¯")
        print(f"ğŸ“ˆ ç»“æœç»Ÿè®¡:")
        print(f"   - ç¡®è®¤çš„åˆ†ç»„: {len(confirmed_groups)}")
        print(f"   - å»ºè®®æ‹†åˆ†: {len(split_suggestions)}")
        print(f"   - å»ºè®®åˆå¹¶: {len(merge_suggestions)}")
        print(f"   - éœ€è¦äººå·¥å®¡æ ¸: {len([x for x in verified_annotations if x.get('verification_status') == 'needs_review'])}")
        
        # è½¬æ¢numpyæ•°æ®ç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹ä»¥æ”¯æŒJSONåºåˆ—åŒ–
        def convert_numpy_types(obj):
            """é€’å½’è½¬æ¢numpyæ•°æ®ç±»å‹ä¸ºPythonåŸç”Ÿç±»å‹"""
            if isinstance(obj, dict):
                return {key: convert_numpy_types(value) for key, value in obj.items()}
            elif isinstance(obj, list):
                return [convert_numpy_types(item) for item in obj]
            elif isinstance(obj, np.float32) or isinstance(obj, np.float64):
                return float(obj)
            elif isinstance(obj, np.int32) or isinstance(obj, np.int64):
                return int(obj)
            elif isinstance(obj, np.bool_):
                return bool(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            else:
                return obj
        
        # ä¿å­˜éªŒè¯æŠ¥å‘Š
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_groups_processed': len(product_groups),
            'confirmed_groups_count': len(confirmed_groups),
            'split_suggestions_count': len(split_suggestions),
            'merge_suggestions_count': len(merge_suggestions),
            'verified_annotations': verified_annotations,
            'split_suggestions': split_suggestions,
            'merge_suggestions': merge_suggestions,
            'processing_summary': {
                'groups_confirmed': confirmed_groups,
                'groups_needing_split': [s['product_id'] for s in split_suggestions],
                'group_pairs_for_merge': [(m['group_a'], m['group_b']) for m in merge_suggestions]
            }
        }
        
        # è½¬æ¢numpyç±»å‹ä»¥æ”¯æŒJSONåºåˆ—åŒ–
        report = convert_numpy_types(report)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # åŒæ—¶ä¿å­˜æ›´æ–°åçš„åŸºç¡€æ ‡æ³¨æ–‡ä»¶
        base_annotations_path = output_path.replace('_verification_report.json', '.json')
        with open(base_annotations_path, 'w', encoding='utf-8') as f:
            json.dump(convert_numpy_types(verified_annotations), f, indent=2, ensure_ascii=False)
        
        print(f"æ›´æ–°åçš„æ ‡æ³¨æ–‡ä»¶å·²ä¿å­˜è‡³: {base_annotations_path}")
        
        return verified_annotations, report


def main():
    """ä¸»å‡½æ•°ï¼šæ‰§è¡Œæ¨¡å‹è§†è§‰éªŒè¯"""
    print("ğŸš€ ä¼˜åŒ–ç‰ˆæ¨¡å‹è§†è§‰éªŒè¯å™¨")
    print("="*50)
    
    # åˆ›å»ºéªŒè¯å™¨å®ä¾‹
    verifier = OptimizedModelVisualVerifier(
        cache_dir="./model_cache/features",
        batch_size=8  # å¯æ ¹æ®GPUå†…å­˜è°ƒæ•´
    )
    
    # æ‰§è¡ŒéªŒè¯ï¼ˆä½¿ç”¨ç°æœ‰æ–‡ä»¶ï¼‰
    annotations_file = "similarity_annotations.json"
    if os.path.exists(annotations_file):
        print(f"\nğŸ“‹ æ‰¾åˆ°æ ‡æ³¨æ–‡ä»¶: {annotations_file}")
        try:
            verified_annotations, report = verifier.verify_similarity_annotations(
                annotations_file=annotations_file
            )
            print(f"\nâœ… éªŒè¯å®Œæˆï¼")
        except FileNotFoundError:
            print(f"âš ï¸  æœªæ‰¾åˆ° {annotations_file}ï¼Œè·³è¿‡å®é™…éªŒè¯")
            print("   æç¤º: å…ˆè¿è¡Œ main2.py ç”Ÿæˆ similarity_annotations.json")
    else:
        print(f"âš ï¸  æœªæ‰¾åˆ° {annotations_file}ï¼Œè·³è¿‡å®é™…éªŒè¯")
        print("   æç¤º: å…ˆè¿è¡Œ main2.py ç”Ÿæˆ similarity_annotations.json")


if __name__ == "__main__":
    main()